package it.univaq.disim.se.masaccio.processor;

import it.univaq.disim.se.masaccio.entity.AccessRequest;
import it.univaq.disim.se.masaccio.entity.IoTRoomData;
import it.univaq.disim.se.masaccio.util.IoTDataDecoder;
import it.univaq.disim.se.masaccio.util.PropertyFileReader;
import kafka.serializer.StringDecoder;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;

import com.datastax.spark.connector.japi.CassandraJavaUtil;

import static com.datastax.spark.connector.japi.CassandraStreamingJavaUtil.javaFunctions;

import java.util.*;



public class AccessRequestProcessor {
	/**
	 * Method to get access request to the database.
	 * 
	 * @param nonfilteredIotDataStream IoT data stream
	 */
	public void processAccessRequests(JavaDStream<AccessRequest> nonfilteredIotDataStream) {
		// Map Cassandra table column
		Map<String, String> columnNameMappings = new HashMap<String, String>();
		columnNameMappings.put("item", "item");
		columnNameMappings.put("cardUid", "carduid");
		columnNameMappings.put("timeStamp", "timestamp");

		// call CassandraStreamingJavaUtil function to save in DB
		javaFunctions(nonfilteredIotDataStream).writerBuilder("primaverakeyspace", "accessrequest",
				CassandraJavaUtil.mapToRow(IoTRoomData.class, columnNameMappings)).saveToCassandra();

		// call CassandraStreamingJavaUtil function to save in DB
		//javaFunctions(nonfilteredIotDataStream).writerBuilder("primaverakeyspace", "numberofpeople",
				//CassandraJavaUtil.mapToRow(IoTRoomData.class, columnNameMappings)).;

	}
}
